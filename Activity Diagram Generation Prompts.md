AI-Driven Architectural Analysis and Documentation of ADABAS Natural SystemsIntroduction: The Challenge of Opaque Legacy Systems and the Promise of AI-Driven AnalysisThe Business ProblemFor decades, applications built with Software AG's ADABAS and Natural have powered mission-critical operations across global enterprises in finance, logistics, and government.1 These systems often encapsulate billions of dollars in business value and represent a unique, battle-tested repository of corporate logic. However, this value is increasingly trapped within an aging technological paradigm. The core challenge facing organizations today is the profound opacity of these legacy systems. They have become "black boxes" for several compounding reasons: the natural loss of institutional knowledge as original developers retire, a historical lack of rigorous documentation standards, and a rapidly shrinking global pool of skilled Natural programmers capable of maintaining and evolving these complex codebases.3 This lack of clarity creates a significant barrier to essential modernization initiatives, inflates maintenance costs, and introduces unacceptable levels of risk into any project that requires modifying or migrating these foundational applications.The AI SolutionThe advent of sophisticated Large Language Models (LLMs) presents a transformative opportunity to address this challenge. The solution proposed herein moves beyond using LLMs as simple code-to-text translators. Instead, it positions them as powerful analytical engines capable of performing a deep, multi-file, cross-referential analysis of an entire ADABAS Natural application portfolio. The objective is to design and engineer prompts that guide an LLM to act as a seasoned mainframe architect. This AI-driven architect will deconstruct the application, trace dependencies between programs and data structures, infer business rules from procedural code, and identify common anti-patterns and technical debt. This process transforms a static, unmanageable, and high-risk codebase into a dynamic, queryable, and comprehensively documented knowledge base, paving the way for informed strategic decisions.Report ObjectivesThis report provides a comprehensive framework for achieving this AI-driven analysis. Its primary objectives are:To deliver two production-ready, model-specific LLM prompts designed to generate deep-dive technical documentation from a complete set of ADABAS Natural source code in a single analytical pass.To provide a third, specialized prompt designed to perform automated quality assurance on the generated documentation, verifying its accuracy against the source code.To support these artifacts with a detailed architectural deconstruction of the ADABAS Natural application ecosystem, providing the necessary context for building and understanding these advanced prompts.Part I: An Architectural Blueprint of an ADABAS Natural ApplicationTo construct a prompt capable of guiding an LLM through a sophisticated analysis, it is imperative to first establish a clear architectural model of the target system. An ADABAS Natural application is not a monolithic entity but a constellation of interacting components. Understanding the role of each component and the syntax that governs their interactions is the foundation upon which a successful AI analysis is built.1.1. The Building Blocks: Programs, Subprograms, and Data AreasThe fundamental units of execution and data definition in a Natural application are distinct objects, each with a specific purpose and relationship to the others. These objects are stored within libraries, which function similarly to file system directories, and are identified by unique names of up to eight characters—a constraint the LLM must recognize to correctly map relationships.7Core ObjectsThe primary executable objects are PROGRAMs and SUBPROGRAMs.2 A PROGRAM is the main entry point for a unit of work and can be executed directly, either in an online (transactional) or batch environment.7 A SUBPROGRAM is a callable module, analogous to a function, procedure, or method in other languages, which cannot be executed directly but must be invoked by another object.10The DEFINE DATA StatementAt the heart of every Natural program or subprogram lies the DEFINE DATA statement. This is a non-executable block at the beginning of the source code that declares all data elements the object will use.12 This includes local variables, parameters passed from other objects, and views of database files. The block begins with DEFINE DATA and concludes with END-DEFINE, and it is within this section that the program's entire data context is established.Data Areas (LDAs, PDAs, GDAs)For modularity and to avoid redundant definitions, Natural encourages the externalization of data definitions into separate, reusable objects known as Data Areas.10 There are three primary types:Local Data Area (LDA): An LDA is an external object containing variable definitions that can be included in a program. It promotes reusability of common data structures across multiple programs without resorting to global variables. It is incorporated into a program using the DEFINE DATA LOCAL USING <LDA_NAME> clause.13Parameter Data Area (PDA): A PDA is a specialized data area that defines the "parameter signature" or "contract" for a SUBPROGRAM or an external SUBROUTINE. It lists the variables that the subprogram expects to receive from its caller. The calling program uses a CALLNAT statement with its own variables, while the subprogram uses a DEFINE DATA PARAMETER USING <PDA_NAME> clause to map those incoming values to its internal parameter definitions.10 This mechanism is central to modular application design.Global Data Area (GDA): A GDA defines a common block of memory that can be shared among multiple, distinct Natural objects within an application. It functions as a shared memory space, allowing different programs to access and modify the same set of variables without passing them as parameters.15 This is a powerful but potentially risky feature that can create tight coupling between components.The relationships between these core components are critical for a complete analysis. The following table provides a quick-reference summary.Table 1: ADABAS Natural Object Cross-Reference| Object Type | Common File Extension | Purpose | Key Interactions & Keywords || :--- | :--- | :--- | :--- || PROGRAM | .NSP | Main executable unit (batch or online). | Invokes subprograms using CALLNAT. Can use LOCAL USING for LDAs and GLOBAL USING for GDAs. Defines database access with VIEW OF. || SUBPROGRAM | .NSS | Callable module; cannot be executed directly. | Invoked by CALLNAT. Defines its interface with PARAMETER USING for PDAs. || DDM | .NSD | Data Definition Module. Logical view of a physical ADABAS file. | Referenced in DEFINE DATA blocks via the VIEW OF <ddm-name> syntax. || LDA | .NSL | Local Data Area. Contains reusable local variable definitions. | Included in a program via DEFINE DATA LOCAL USING <lda-name>. || PDA | .NSP | Parameter Data Area. Defines the parameters for a subprogram. | Referenced in a subprogram via DEFINE DATA PARAMETER USING <pda-name>. || GDA | .NSG | Global Data Area. Defines shared data for multiple objects. | Included in programs via DEFINE DATA GLOBAL USING <gda-name>. || MAP | .NSM | Defines screen layouts for online (3270-style) applications. | Used with statements like INPUT USING MAP. |A fundamental challenge in analyzing Natural code is that a program's complete data context is rarely self-contained within its own source file. The extensive use of USING clauses means the full data definition is distributed across multiple, interdependent files.9 For example, when an LLM analyzes a program PROG1.NSP and encounters the statement DEFINE DATA LOCAL USING MYLDA, it has no way of knowing the structure of the variables defined in MYLDA by looking at PROG1.NSP alone. Similarly, a CALLNAT 'SUB1' USING #PARAM1 #PARAM2 statement is ambiguous without understanding the data types of #PARAM1 and #PARAM2 and the corresponding parameter definitions within the SUB1 subprogram, which are likely defined in a PDA.This is analogous to a C compiler needing header files to understand function prototypes or a linker needing object files to resolve external symbols. Therefore, a naive, single-file analysis will invariably fail to produce a complete or accurate picture. The analysis prompt must explicitly instruct the LLM to act as a "linker." It must be given a clear directive: "When you encounter a USING <name> clause (whether LOCAL, PARAMETER, or GLOBAL), you must find the corresponding source file for <name> (e.g., MYLDA.NSL, MYPDA.NSP) within the provided set of files, parse its structure, and logically integrate those variable definitions into your analysis of the calling program before proceeding." This "linker" capability is the single most critical instruction for enabling a deep, multi-file analysis.1.2. The Data Dictionary: The Role of Data Definition Modules (DDMs)The bridge between the Natural application logic and the physical ADABAS database is the Data Definition Module (DDM). A DDM is not the data itself, nor is it the physical database schema; it is a purely logical, application-level view of an ADABAS file that the Natural program uses to interact with the database.19A DDM serves several critical functions:Mapping and Abstraction: It maps the user-friendly, descriptive field names used within a Natural program (e.g., CUSTOMER-LAST-NAME) to the cryptic, two-character physical field names required by the ADABAS database (e.g., C1).19 This provides a vital layer of abstraction.Data Typing and Formatting: It defines the format (e.g., Alphanumeric (A), Packed Decimal (P), Numeric (N), Date (D)) and length of each field that the program will work with.22Defining Complex Structures: It describes ADABAS's unique and powerful denormalization structures, such as Multiple-value Fields (MUs), which allow a single field to have up to 191 values within a single record, and Periodic Groups (PEs), which are repeating groups of fields.3 Understanding these is essential to correctly interpret program logic that processes them.Identifying Keys: It specifies which fields are Descriptors, which are key fields indexed by ADABAS for fast retrieval. This is crucial for understanding database access performance.19Within a program's DEFINE DATA block, a DDM is referenced as a VIEW. For instance, the statement 01 EMPLOYEES-VIEW VIEW OF EMPLOYEES declares a data structure named EMPLOYEES-VIEW whose layout is defined by the DDM named EMPLOYEES.NSD.7The DDM represents the formal "data contract" between the application logic and the database. All data manipulation statements (READ, FIND, UPDATE, STORE, DELETE) are governed by the definitions within this contract. A statement like READ EMPLOYEES-VIEW BY NAME is functionally meaningless without access to the EMPLOYEES DDM. The DDM informs the analyst (and the LLM) that NAME is a descriptor field, which explains why a READ BY statement is an efficient way to access the data, as it uses the database's inverted list (index).19 If the DDM indicates that PHONE-NUMBER is a Multiple-value field, it immediately clarifies why the program might contain a loop to process multiple phone numbers for a single employee record.Consequently, the analysis prompt must instruct the LLM to treat the DDM as the ultimate source of truth for all database interactions. It must be tasked with building a comprehensive "Data Dictionary" section in its output. This requires parsing every provided DDM file and creating a detailed catalog for each, listing every field's programmatic name, its corresponding 2-character physical name, its data format and length, and any special properties (e.g., Descriptor, MU, PE). This dictionary becomes an indispensable reference for understanding the program's data manipulation logic.1.3. The Control Flow: Database Access and Inter-Program CommunicationThe procedural logic of a Natural program is defined by its control flow statements, particularly those that handle database access and communication between different program objects.Database Processing LoopsNatural provides several powerful statements for database access, each optimized for a different use case:READ: This statement is used for sequential processing of records. It is most often used with a BY <descriptor> clause to read records in the order of a specific key (e.g., READ EMPLOYEES-VIEW BY NAME).7FIND: This statement is used to select a specific set of records based on one or more search criteria, often complex. Unlike READ, which processes sequentially, FIND identifies all qualifying records first and then presents them to the program, typically in order of their Internal Sequence Number (ISN).7HISTOGRAM: This is a highly efficient statement that reads values directly from the ADABAS inverted list (the index) for a given descriptor field. It is used when the program only needs the list of unique values for a field or the count of records for each value, without needing to access the full data records themselves. This is a common pattern for populating dropdown lists or performing validation checks.25Control-Break LogicA classic feature of mainframe report programming is control-break processing, implemented in Natural with the AT BREAK statement.27 This non-procedural statement is used within a database processing loop (READ, FIND, etc.) and triggers a block of code whenever the value of a specified "control field" changes from one record to the next. For example, when reading employee records sorted by department, an AT BREAK ON DEPARTMENT block would execute after the last employee of one department and before the first employee of the next, making it the ideal place to print departmental subtotals.Inter-Program CallsThe modern and standard mechanism for invoking a SUBPROGRAM is the CALLNAT statement.11 It transfers control to the named subprogram and can pass parameters to it. Parameters are defined in the subprogram's PDA and can be passed by reference (the default, where the subprogram can modify the original variable) or by value (where the subprogram works on a copy).30The choice of these specific statements by a developer is not arbitrary; it is a strong indicator of the underlying business function being implemented.25 A program that uses an AT BREAK ON DEPT-ID block containing a COMPUTE #DEPT-TOTAL = #DEPT-TOTAL + #SALARY statement is unambiguously implementing a departmental salary summation report. A HISTOGRAM ON COUNTRY-CODE is almost certainly being used to populate a list of valid countries for a user interface dropdown or to perform an input validation check. A FIND statement with multiple WITH clauses directly corresponds to a search screen with multiple input fields.This realization is key to elevating the AI-generated documentation from a simple code summary to a genuine business analysis. The prompt must instruct the LLM to act not just as a programmer but as a business analyst. It must be tasked with inferring the business rule or user story from the technical code patterns it observes. The final documentation should contain a "Business Logic" section that translates the technical implementation into plain business language, for example: "This program generates a departmental salary report. It reads all employee records sorted by department. Upon detecting a change in department, it calculates and displays the total salary for the preceding department."1.4. Identifying Technical Debt and Common Anti-PatternsA truly deep-dive analysis does not merely document what is present; it also assesses the quality, risk, and maintainability of the code. This involves identifying instances of technical debt—design shortcuts and outdated practices that increase the long-term cost of ownership.32 In ADABAS Natural, several common anti-patterns are strong indicators of technical debt.Lack of Application-Enforced Referential Integrity: ADABAS, being a pre-relational inverted-list database, does not natively enforce referential integrity constraints like foreign keys.7 If a parent record is deleted, it is entirely the application's responsibility to find and delete all associated child records in other files. A DELETE statement in one program without corresponding DELETE logic for dependent records is a major risk, potentially leading to orphaned data.The REDEFINE Clause: Natural provides a REDEFINE clause within the DEFINE DATA block, which allows a single area of memory to be interpreted in multiple different ways (e.g., treating a 10-byte alphanumeric field as a 4-byte number followed by a 6-byte string).35 While powerful, this feature creates extremely tight coupling between the data structure and the logic that uses it. It is a significant source of technical debt, making the code brittle and difficult to understand, and can easily lead to data corruption if the underlying structure is ever modified without updating all redefinitions.Inefficient Database Access: Certain database access patterns are known to be inefficient. For example, using a READ LOGICAL loop to process a very large percentage of a file is much less performant than using the READ PHYSICAL statement, which is designed for bulk sequential reads.25 Similarly, nesting a FIND or READ loop inside another database processing loop can lead to the classic "N+1 query problem," causing a storm of database calls and poor performance.Hard-Coded Values: Embedding "magic" numbers or strings directly into procedural logic (e.g., IF #STATUS-CODE = 'A' THEN...) obscures their business meaning and makes the application difficult to maintain.33 If the meaning of 'A' changes, developers must hunt for every instance throughout the codebase.To produce an expert-level assessment, the analysis prompt must imbue the LLM with the persona of a "Forensic Code Analyst" or "Senior Architect." Standard documentation describes the "as-is" state; expert documentation provides an "as-is" assessment. The prompt must contain a specific set of rules for the LLM to apply during its analysis. For example, it should be explicitly instructed to create a "Technical Debt & Risk Analysis" section and be given directives such as: "When you encounter a DELETE statement for a record in a given DDM, search all other provided code for related DELETE statements on logically dependent DDMs. If none are found, flag a potential 'Orphaned Record Risk' due to a lack of application-enforced referential integrity." Another rule would be: "Identify every use of the REDEFINE keyword. Flag each instance as high-risk technical debt, document precisely which fields are being redefined and how, and explain the potential for data misalignment or maintenance issues." This approach transforms the LLM from a documentarian into a strategic risk assessor.Part II: Prompt Architectures for Automated Code DocumentationThis section presents the core deliverables of this report: two distinct, heavily annotated prompts designed to orchestrate the AI-driven analysis of ADABAS Natural source code. The first is tailored for a large, general-purpose model like OpenAI's GPT-4.1, focusing on narrative and structural explanation. The second is designed for a hypothetical 'o3' reasoning model, prioritizing structured, verifiable data output suitable for machine consumption.2.1. Prompt for GPT-4.1: A Comprehensive Narrative and Structural AnalysisThis prompt is engineered to leverage the strong narrative and summarization capabilities of models like GPT-4.1. It guides the model to produce a detailed, human-readable technical specification document that explains the application's architecture, business logic, and potential risks in clear prose, supported by structured tables and diagrams.ROLE AND GOALYou are an expert Senior Mainframe Modernization Architect specializing in ADABAS Natural. Your primary goal is to analyze the provided set of interconnected ADABAS Natural source code files and produce a single, comprehensive, and human-readable technical documentation document. Your analysis must be deep, cross-referencing information between files to build a complete picture of the application's architecture, data structures, business logic, and dependencies.CONTEXT: ADABAS NATURAL APPLICATION STRUCTURETo perform your analysis, you must understand the following core components and their relationships:PROGRAM (.NSP): A main executable object.SUBPROGRAM (.NSS): A callable module invoked via CALLNAT.DDM (.NSD): A Data Definition Module, which is the logical view of an ADABAS database file. It is referenced in programs with VIEW OF <ddm-name>. It maps user-friendly program field names to short physical database field names and defines data types (A-Alphanumeric, P-Packed, N-Numeric, etc.) and special properties like Descriptors (keys), Multiple-value fields (MU), and Periodic Groups (PE).LDA (.NSL): A Local Data Area, an external file containing reusable variable definitions, included with DEFINE DATA LOCAL USING <lda-name>.PDA (.NSP): A Parameter Data Area, defining the parameters for a subprogram, included with DEFINE DATA PARAMETER USING <pda-name>.GDA (.NSG): A Global Data Area, defining data shared across multiple objects, included with DEFINE DATA GLOBAL USING <gda-name>.INPUT FORMATYou will be provided with a single text block containing a collection of concatenated source code files. Each individual file within this block is clearly demarcated. Each file begins with a line --- START OF FILE: <filename> --- and ends with a line --- END OF FILE: <filename> ---. The filename (e.g., PROG1.NSP, EMPL-V.NSD, PARM1.NSP) indicates the object's name and type.ANALYSIS INSTRUCTIONS & "LINKER" LOGICYour analysis must be holistic. Do not analyze files in isolation. Follow these critical steps:Inventory Scan: First, perform a complete scan of all --- START OF FILE:... --- markers to create an inventory of all programs, subprograms, DDMs, and data areas provided in the input. This inventory is your map for the entire analysis.Act as a "Component Linker": For each PROGRAM or SUBPROGRAM you analyze, you must resolve its external dependencies.When you parse a DEFINE DATA block and encounter a USING <name> clause (e.g., LOCAL USING MYLDA, PARAMETER USING MYPDA), you must immediately locate the source code for <name> from your inventory (e.g., MYLDA.NSL, MYPDA.NSP).Parse the variable definitions within that data area and logically integrate them into the data definition of the program you are currently analyzing. This is essential for a correct understanding of the available variables.Enforce the "Data Contract": The DDM is the source of truth for all database information.When you encounter a VIEW OF <ddm-name> statement, you must locate the source for <ddm-name>.NSD from your inventory.For all subsequent database statements (READ, FIND, UPDATE, etc.) that use this view, you must reference the DDM to understand the fields, their true names, formats, lengths, and properties (Descriptor, MU, PE).You will compile this information into the "Data Dictionary" section of your output.Infer "Business Logic": Do not just describe the code; interpret its purpose.When you see specific code patterns, infer the business rule they implement.Example 1: An AT BREAK ON DEPT-ID block that sums salaries should be described as: "This section implements a departmental salary summation report."Example 2: A HISTOGRAM ON COUNTRY-CODE statement should be described as: "This logic retrieves a list of all unique country codes, likely for input validation or to populate a user selection list."Example 3: A FIND with multiple WITH clauses should be described as: "This corresponds to a multi-field search function."Act as a "Forensic Code Analyst": Proactively identify and report on technical debt and architectural risks.REDEFINE Risk: Identify every use of the REDEFINE keyword. In the "Technical Debt" section, list each redefinition, specifying the file, the base field, and how it is being redefined. Explain the risk of data misalignment and maintenance complexity.Referential Integrity Risk: For every DELETE statement, analyze the application to see if corresponding DELETEs are issued for logically dependent child records. If not, flag a potential "Orphaned Record Risk" due to the lack of application-enforced referential integrity.Performance Anti-Patterns: Identify and flag inefficient database access patterns, such as READ or FIND loops nested inside other database loops (N+1 problem).Hard-Coded Values: Identify business-critical logic that depends on hard-coded literals (e.g., IF #STATUS = 'A'). List these in the "Technical Debt" section and recommend they be replaced with named constants.OUTPUT STRUCTUREProduce your final analysis in a single, well-formatted Markdown document. Adhere strictly to the following structure:Technical Specification: [Main Program Name]1. Executive SummaryA high-level overview of the main program's function. What is its primary business purpose? What are its key inputs and outputs?2. System ComponentsA table listing all source files identified in the input.| Filename | Object Type | Description || :--- | :--- | :--- ||... |... |... |3. Data DictionaryFor each DDM file provided, create a subsection.DDM:Database ID:File Number: [File Number]Field Mappings:| Program Field Name | Physical Name | Format | Length | Notes (Descriptor, MU, PE, etc.) || :--- | :--- | :--- | :--- | :--- ||... |... |... |... |... |4. Main Program Analysis: [Program Name]4.1. Business PurposeA detailed narrative of the program's business logic, inferred from the code. Explain what the program accomplishes from a user's perspective.4.2. Data DefinitionsLocal Data: Summary of key variables defined directly or via LDAs.Global Data: Summary of GDA variables used.Database Views: List of DDMs used via VIEW OF statements.4.3. Control Flow & Processing LogicA step-by-step narrative of the program's execution.Initialization: Describe initial setup and variable assignments.Main Processing Loop: Describe the primary READ, FIND, or HISTOGRAM loop. Explain the selection criteria and processing order.Conditional Logic: Explain key IF/ELSE or DECIDE ON blocks and the business rules they represent.Subprogram Calls: Detail each CALLNAT statement, the subprogram being called, and the parameters being passed.Output Generation: Describe how the final output (report, screen, etc.) is generated.4.4. CRUD AnalysisA table summarizing all database interactions.| DDM / Database File | Create (STORE) | Read (READ/FIND/HISTOGRAM) | Update (UPDATE) | Delete (DELETE) || :--- | :--- | :--- | :--- | :--- ||... | [Yes/No] | [Yes/No] | [Yes/No] | [Yes/No] |5. Subprogram Analysis:(Repeat this entire section for each SUBPROGRAM identified)5.1. Business Purpose...5.2. Data Definitions (Parameters)...5.3. Control Flow & Processing Logic...5.4. CRUD Analysis...6. Dependency Analysis6.1. Call Graph (Mermaid Syntax)Provide a Mermaid.js graph diagram showing the call relationships.mermaidgraph TD;ProgramA --> SubprogramB;ProgramA --> SubprogramC;SubprogramB --> SubprogramD;
#### **6.2. Data Flow Overview**
Describe the high-level flow of data through the application. Where does data originate (e.g., user input, database read)? How is it transformed? What are the final outputs (e.g., database update, report)?

### **7. Technical Debt & Risk Analysis**
A bulleted list of all identified risks and anti-patterns.
- **Risk Type:** (e.g., REDEFINE Usage)
  - **Location:** `[Filename:LineNumber]`
  - **Description:** A clear explanation of the issue and its potential impact.
  - **Recommendation:** A suggestion for remediation.
- **Risk Type:** (e.g., Potential Orphaned Record Risk)
  -...

---
2.2. Prompt for the 'o3' Reasoning Model: A Logic-Centric and Dependency-Focused AnalysisThis prompt is designed for a hypothetical reasoning-optimized model, referred to as 'o3'. The primary goal is not narrative but the extraction of structured, verifiable data. The output is intended to be machine-readable (JSON, Mermaid.js), forming a knowledge base that can be ingested by other tools for further analysis, visualization, or governance.ROLE AND GOALYou are a logical code analysis engine. Your goal is to deconstruct the provided set of interconnected ADABAS Natural source code files into a series of structured data outputs (JSON, Mermaid.js). Your analysis must be precise, verifiable, and focused on representing the application's architecture, logic, and dependencies in a machine-readable format.CONTEXT: ADABAS NATURAL APPLICATION STRUCTUREYou must parse the following object types based on their file extensions and interrelationships:Executables: PROGRAM (.NSP), SUBPROGRAM (.NSS)Data Definitions: DDM (.NSD), LDA (.NSL), PDA (.NSP), GDA (.NSG)Key Linkages:CALLNAT links a caller to a SUBPROGRAM.USING clauses link an executable to a Data Area (LDA, PDA, GDA).VIEW OF links an executable to a DDM.INPUT FORMATYou will be provided with a single text block containing multiple concatenated source code files. Each file is demarcated by --- START OF FILE: <filename> --- and --- END OF FILE: <filename> ---.ANALYSIS INSTRUCTIONSInventory and Link: Scan all files to build an internal map of all components. For each executable, resolve all USING and VIEW OF clauses by linking to the corresponding data definition files before parsing the procedural code. This linked data context is mandatory for accurate analysis.Extract and Structure: Process the code with the sole purpose of extracting the specific information required by the OUTPUT STRUCTURE below. Do not generate prose or explanations unless explicitly requested. Your output must be valid JSON or Mermaid.js syntax.OUTPUT STRUCTUREGenerate your output as a single Markdown file containing the following sections, each with its own code block.**1. Component Inventory (JSON)**json{"components":}
### **2. Data Dictionary (JSON)**
```json
{
  "dataDictionary":
        },
        {
          "programName": "SALARY",
          "physicalName": "S1",
          "format": "P",
          "length": 7.2,
          "attributes": ["MU"]
        }
      ]
    }
  ]
}
3. Call Graph (Mermaid.js Syntax)Code snippetgraph TD;
    PROG1 --> SUB1;
    PROG1 --> SUB2;
    SUB2 --> SUB3;
4. Data Lineage Map (JSON)JSON{
  "dataLineage":,
      "sink": "REPORT_FILE.FIELD_A"
    }
  ]
}
5. Business Rules Formalization (JSON)JSON{
  "businessRules":
}
6. Technical Debt Register (JSON)JSON{
  "technicalDebt":
}

The two-prompt approach is deliberate. The GPT-4.1 prompt is optimized for immediate human consumption, ideal for project managers and business analysts. The 'o3' prompt creates a structured, machine-readable asset, perfect for architects and developers looking to integrate this analysis into a larger automated toolchain.

**Table 2: Prompt Feature Comparison (GPT-4.1 vs. 'o3')**
| Feature | GPT-4.1 Approach (Narrative) | 'o3' Approach (Structured) | Rationale |
| :--- | :--- | :--- | :--- |
| **Business Logic** | Explains the "why" in clear, narrative prose within the "Business Purpose" section. | Formalizes rules into a JSON structure with `trigger`, `condition`, and `action` keys. | The narrative is easier for non-technical stakeholders to understand. The JSON structure is verifiable and can be used for automated rule validation. |
| **Dependency Mapping** | Generates a Mermaid.js diagram for visual representation and describes data flow in a narrative. | Generates a Mermaid.js graph for the call stack and a separate, detailed JSON for data lineage, tracing fields from source to sink. | The visual graph is useful for high-level understanding. The detailed JSON lineage map is required for impact analysis and automated dependency tracking. |
| **Data Dictionary** | Presents the DDM information in a human-readable markdown table. | Encodes the DDM information in a structured JSON array, allowing for programmatic querying. | The table is good for reports. The JSON is an asset that can be loaded into a metadata repository or CMDB. |
| **Technical Debt** | Describes risks in a bulleted list with explanations and recommendations. | Creates a formal "register" in JSON, with fields for `debtId`, `severity`, `location`, and `recommendation`. | The narrative is good for risk assessment meetings. The JSON register can be imported into issue tracking systems like Jira to create actionable tickets automatically. |

## Part III: A Framework for Quality Assurance of Generated Documentation

Generating documentation automatically is only half the battle. The output must be accurate, complete, and trustworthy. A flawed document can be more dangerous than no document at all, as it can lead to incorrect assumptions and flawed decision-making. This section provides a framework for ensuring the quality of the AI-generated output, centered around a formal evaluation rubric and a specialized prompt that automates the technical review process.

### 3.1. The Rubric of Excellence: Defining Quality in Legacy Documentation

Before evaluation can occur, a clear standard of quality must be established. Good documentation for a legacy system is not just a summary of the code; it is a complete, accurate, and insightful guide for future developers, architects, and business analysts.[38, 39] The following rubric breaks down this quality standard into verifiable criteria.

**Table 3: Documentation Quality Evaluation Rubric**
| Category | Criteria | Verification Method |
| :--- | :--- | :--- |
| **Completeness** | Are all provided source files (programs, subprograms, DDMs, data areas) identified in the component list? | Compare the "System Components" table in the documentation against the list of files provided as input. |
| | Are all `CALLNAT` statements represented in the Call Graph? | For every `CALLNAT <subprogram>` statement in the source code, verify a corresponding arrow exists in the Mermaid.js graph in the documentation. |
| | Are all database files (`VIEW OF <ddm>`) that are accessed by the code listed in the CRUD analysis? | For every DDM referenced in a `VIEW OF` statement, check that it has a row in the "CRUD Analysis" table. |
| **Accuracy** | Is the Data Dictionary correct? (Field names, physical names, formats, lengths) | For a sample of DDMs, compare the "Field Mappings" table in the documentation against the source `.NSD` file. Verify program name, physical name, format, and length for each field. |
| | Is the parameter mapping for `CALLNAT` calls described correctly? | For a sample `CALLNAT` call, compare the parameters listed in the documentation against the variables in the `CALLNAT` statement and the corresponding `DEFINE DATA PARAMETER` block in the called subprogram. |
| | Does the CRUD analysis accurately reflect the operations performed? | For a sample program, find a `READ`, `UPDATE`, `STORE`, or `DELETE` statement in the source code and verify that the corresponding cell in the "CRUD Analysis" table is marked "Yes". |
| **Clarity** | Is the inferred "Business Purpose" a logical and clear explanation of the code's function? | A human expert should read the "Business Purpose" section and judge if it accurately reflects the overall function of the program logic (loops, conditions, outputs). |
| | Is the "Control Flow & Processing Logic" narrative easy to follow and does it match the program's structure? | Trace the execution path in the source code and compare it to the step-by-step narrative in the documentation. Ensure major loops and conditional branches are correctly described. |
| **Insightfulness** | Have all instances of high-risk patterns like `REDEFINE` been identified? | Search the entire source code for the `REDEFINE` keyword. Verify that each instance is listed and explained in the "Technical Debt & Risk Analysis" section. |
| | Is the assessment of potential referential integrity issues logical and well-supported? | Review the explanation for any "Orphaned Record Risk" and check the source code to confirm the described `DELETE` logic (or lack thereof). |

### 3.2. The Evaluation Prompt: An Automated Technical Reviewer

This prompt operationalizes the rubric, turning the LLM into a meticulous quality assurance architect. It takes the source code, the generated documentation, and the rubric as inputs and produces a concise report detailing any discrepancies. This automates the most time-consuming aspects of technical review.

# ROLE AND GOAL

You are a meticulous and exacting Quality Assurance Architect. Your sole function is to review a piece of AI-generated Technical Documentation against the original source code from which it was derived. You must use the provided Evaluation Rubric as your guide. Your goal is to identify and report any errors, omissions, or inconsistencies in the documentation.

# INPUTS

You will be provided with three distinct inputs, each enclosed in a markdown block:
1.  **``**: The complete, concatenated ADABAS Natural source code, with each file demarcated by `--- START/END OF FILE ---` markers. This is the ground truth.
2.  **``**: The technical documentation produced by a previous AI model. This is the document under review.
3.  **``**: A table defining the quality criteria and verification methods you must follow.

# INSTRUCTIONS

You must systematically proceed through each item in the ``. For each criterion, you will perform the specified verification method by cross-referencing the `` with the ``.

-   **Be Precise:** When you find a discrepancy, you must be specific. Cite the exact filename and, if possible, the line number from the `` that contradicts the information in the ``.
-   **Example Verification for "Completeness - Call Graph":**
    1.  Scan the entire `` and extract a list of all `CALLNAT` statements, noting the calling program and the called subprogram.
    2.  Parse the Mermaid.js syntax in the "Call Graph" section of the ``.
    3.  Compare the two lists. If a `CALLNAT` from the source code is missing from the graph, it is a 'FAIL'. If the graph shows a call that does not exist, it is a 'FAIL'.
-   **Example Verification for "Accuracy - Data Dictionary":**
    1.  Select a DDM from the documentation, for example, `EMPLOYEE.NSD`.
    2.  Locate the `--- START OF FILE: EMPLOYEE.NSD ---` block in the ``.
    3.  Compare each field's definition in the source (e.g., `02 AA (A,8) / 'PERSONNEL-ID'`) with the corresponding row in the documentation's "Field Mappings" table.
    4.  Verify that the Program Name (`PERSONNEL-ID`), Physical Name (`AA`), Format (`A`), and Length (`8`) all match. Any mismatch is a 'FAIL'.
-   **Example Verification for "Insightfulness - REDEFINE Risk":**
    1.  Perform a global search for the keyword `REDEFINE` across the entire ``.
    2.  For each match found, verify that there is a corresponding entry in the "Technical Debt & Risk Analysis" section of the ``. Any missing entry is a 'FAIL'.

# OUTPUT FORMAT

Produce your findings as a concise QA Report in Markdown. Your report must follow this exact format. For each item from the rubric, you must state 'PASS' or 'FAIL'. For every 'FAIL', you must provide a bulleted list of the specific discrepancies you found.

---

## **Quality Assurance Report**

### **1. Completeness**
-   **Component List:**
    -   *(If FAIL, list missing/extra files here)*
-   **Call Graph:**
    -   *FAIL: The `CALLNAT` from `PROG1.NSP` to `SUB3.NSS` is missing from the documentation's call graph.*
-   **CRUD Analysis:**

### **2. Accuracy**
-   **Data Dictionary:**
    -   *FAIL: In DDM `EMPLOYEE.NSD`, the documentation lists the length of field `FULL-NAME` as (A,40), but the source code at line 15 defines it as (A,50).*
-   **Parameter Mapping:**
-   **CRUD Operations:**

### **3. Clarity**
-   **Business Purpose:**
    -   *FAIL: The business purpose for `PROG2.NSP` describes a reporting function, but the code primarily contains `UPDATE` and `DELETE` logic, suggesting it is a data maintenance module, not a report generator.*
-   **Control Flow:**

### **4. Insightfulness**
-   **REDEFINE Identification:**
    -   *FAIL: The `REDEFINE` of field `#WORK-AREA` in file `SUB2.NSS` at line 42 is not documented in the Technical Debt section.*
-   **Referential Integrity Analysis:**

---

## Conclusion: Implementing and Extending AI-Assisted Documentation

### Summary of the Approach

The framework detailed in this report presents a strategic, engineered approach to legacy system documentation. It moves beyond simplistic code summarization by employing context-rich, persona-driven prompts that transform a Large Language Model into a specialized ADABAS Natural analyst. The core of this strategy lies in providing the LLM with the architectural context of the Natural ecosystem and giving it explicit instructions to perform complex analytical tasks. The "Component Linker" logic enables a true multi-file analysis by resolving external data dependencies, while the "Forensic Code Analyst" persona elevates the output from mere documentation to a critical assessment of technical debt and architectural risk. This methodology creates documentation that is not only descriptive but also diagnostic.

### Practical Implementation

The successful implementation of this AI-assisted documentation process follows a clear workflow:

1.  **Gather Artifacts:** Collect all relevant source code files for the application being analyzed. This includes all `PROGRAM`s, `SUBPROGRAM`s, `DDM`s, `LDA`s, `PDA`s, and `GDA`s that constitute the complete application.
2.  **Prepare Input:** Concatenate all collected source files into a single text file. Each file must be clearly demarcated with the `--- START OF FILE: <filename> ---` and `--- END OF FILE: <filename> ---` markers as specified in the prompts.
3.  **Generate Documentation:** Execute the chosen generation prompt (the GPT-4.1 prompt for a narrative document or the 'o3' prompt for structured data) with the concatenated source code as its input.
4.  **Evaluate Quality:** Execute the evaluation prompt. Provide it with the original source code, the newly generated documentation, and the evaluation rubric.
5.  **Review and Refine:** The output of the evaluation prompt will be a QA report highlighting any discrepancies. A human subject matter expert should review this report, validate the flagged issues, and make any necessary final corrections to the documentation. This human-in-the-loop step ensures the final product is 100% trustworthy.

### Future Directions

The documentation generated by this process should not be viewed as a static, final product. It is a foundational digital asset that can be leveraged in a broader modernization toolchain. The highly structured JSON output from the 'o3' reasoning model is particularly valuable for this purpose. It can be used to:

*   **Automate CMDB Population:** The component inventory, data dictionary, and dependency information can be programmatically ingested to populate or update a Configuration Management Database, providing an accurate, live view of the legacy application portfolio.
*   **Power Advanced Analysis:** The call graphs and data lineage maps can be loaded into a graph database (like Neo4j) to enable complex queries for impact analysis (e.g., "Show me all programs that will be affected by a change to this database field").
*   **Accelerate Modernization and Testing:** The formalized business rules and technical debt register provide a clear roadmap for refactoring efforts. The detailed data definitions and logic flows can be used as a blueprint for generating automated test cases, ensuring that any modernized equivalent of the application maintains functional parity with the original.[38, 40]

By implementing this AI-driven framework, organizations can effectively peel back the layers of their most opaque legacy systems, mitigating risk, empowering developers, and transforming a significant liability into a well-understood strategic asset ready for the future.
